---
title: "Homework 3"
format: 
    html:
        embed-resources: true
---


__Due Date:__ 2022-10-30 at 8:30 AM PT
---


__Name:__ Sabahat


## Preparation

1. Create a 'code' folder in the root directory of your repository.
1. Inside the 'code' folder, create a file '01_clean_data.\<your_extension_here\>'.
Your extension should be the one you use for your programming language of choice (e.g., '.R' for R, '.py' for Python, '.jl' for Julia).
1. Copy any code from HW_02 you need to subset and merge the NRI and SVI datasets into the '01_clean_data' file.
1. Add a 'processed' directory to the 'data' folder.
1. Add a line at the end of the file that saves the merged dataset to 'data/processed' directory.
1. Run the '01_clean_data' file to ensure that the merged dataset runs and creates the proper file.
1. Add and commit the '01_clean_data' file to the repository.



## Homework - Principal Component Analysis

The CDC Social Vulnerability Index (SVI) takes multiple differen population-level inputs (e.g., % of the population living in poverty, % of the population without health insurance) to identify particularly vulnerable counties.
While the CDC SVI scores rely on adding up the percentiles of various characteristics, there are alternative indexes (e.g., [University of South Carolina SoVI index](https://sc.edu/study/colleges_schools/artsandsciences/centers_and_institutes/hvri/data_and_resources/sovi/index.php)) that use methods like PCA.
Here, we are going to use the CDC SVI data to create an alternative index based on PCA.

1. The following variables are used in the SVI:
`EP_POV150, EP_UNEMP, EP_HBURD, EP_NOHSDP, EP_UNINSUR, EP_AGE65, EP_AGE17, EP_DISABL, EP_SNGPNT, EP_LIMENG, EP_MINRTY, EP_MUNIT, EP_MOBILE, EP_CROWD, EP_NOVEH, EP_GROUPQ, EP_NOINT`
    a. Subset the merged dataset to only include the variables above and look at the pattern of missing data.
    Are missing observations scattered throughout the data or are entire rows or columns missing?
    b. PCA cannot handle missing values by default.
    There are several options for handling missing data generally, including imputation, removing rows with missing data, or removing columns with missing data.
    Deal with the missing data in a way that makes sense for the pattern of missing data and the goals of the analysis. Explain why you made this decision.
    _Note: How you handle this is specific to the missing data pattern and the goals of the analysis.
    For example, when entire rows or columns are missing, imputation may not be appropriate and dropping those rows or columns is usually the best option.
    Conversely, if you have a general missingness pattern where missing observations are scattered throughout the data, imputation is likely the best option._
    a. After dealing with the missing data, perform PCA on the SVI variables.

```{python}
import pandas as pd
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import numpy as np
from joblib import Parallel, delayed
```
    
# Subset the dataset to include only the SVI variables
```{python}
svi_variables = ['EP_POV150', 'EP_UNEMP', 'EP_HBURD', 'EP_NOHSDP', 'EP_UNINSUR', 'EP_AGE65',
                 'EP_AGE17', 'EP_DISABL', 'EP_SNGPNT', 'EP_LIMENG', 'EP_MINRTY', 'EP_MUNIT',
                 'EP_MOBILE', 'EP_CROWD', 'EP_NOVEH', 'EP_GROUPQ', 'EP_NOINT']

file_path = '../data/processed/merged_dataset.csv'  

# Load the CSV data with selected columns
svi_data = pd.read_csv(file_path, usecols=svi_variables)

# Check the missing data in each column
missing_summary = svi_data.isnull().sum()
print(missing_summary)
```
```{python}
# Check for observations with all missing values
total_missing_per_row = svi_data.isnull().sum(axis=1)
missing_all_data = svi_data[total_missing_per_row == len(svi_variables)]
print(f"Number of samples with all missing values: {missing_all_data.shape[0]}")
print(missing_all_data)
```
# The output shows that 96 rows conatain missing data so these rows will be removed in next step.
```{python}
# Drop rows with missing data
svi_data = svi_data.dropna()
svi_data.isnull().sum().sum() 
# to confirm all missing rows are dropped 
```

#a. After dealing with the missing data, perform PCA on the SVI variables.
```{python}
# Standardize the data before PCA to get all the columns values within the same range N(0,1)
scaler = StandardScaler()
standardized_data = scaler.fit_transform(svi_data)
```

# Perform PCA on the standardized SVI variables
```{python}
pca = PCA()  
principal_components = pca.fit_transform(standardized_data)
# Create a DataFrame with the principal components to take a better look at few samples
pca_df = pd.DataFrame(data=principal_components)
print(pca_df.head())

# Plot the PCA results (PC 1&2 only) 
plt.figure(figsize=(8, 6))
plt.scatter(pca_df[0], pca_df[1], alpha=0.5)
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('PCA of SVI Variables')
plt.grid()
plt.show()
```

1. Plot the eigenvectors or loadings associated of the first three principal components.
Make sure that the axis labels correspond to the variable names and not the indices of the variables.
How would you interpret the first three prinicpal components?
_Note: you can find the documentation for the SVI variables [here](https://www.atsdr.cdc.gov/placeandhealth/svi/documentation/SVI_documentation_2022.html)._
```{python}
# Plot the loadings (eigenvectors) for the first three principal components
loadings = pca.components_
x_labels = svi_variables

plt.figure(figsize=(15, 6))
for i in range(3):
    plt.subplot(1, 3, i + 1)
    plt.bar(x_labels, loadings[i], alpha=0.7)
    plt.xticks(rotation=90)
    plt.xlabel('SVI Variables')
    plt.ylabel('Loading Value')
    plt.title(f'Eigenvector Loadings for PC{i+1}')
plt.tight_layout()
plt.show()
```

Answer.  PC1 captures the main pattern of economic indicators moving together, such as unemployment, poverty, low literacy levels, lack of insurance, and single parenting. These economic indicators are strongly correlated, suggesting that areas with high scores on PC1 face significant economic challenges and social vulnerabilities. Elderly Population (65+) and housing in Large Structures groups contribute negatively, indicating that they are less affected by these economic stressors. This might be due to stable income sources like pensions or secure housing situations. Similalry, housing in buildings with 10 or more units also shows a negative contribution, suggesting these structures might provide more stable or affordable living conditions, reducing economic stress.

PC2 highlights significant contributions from variables such as age 65+, disability, the percentage of mobile homes, and households without internet access.These factors suggest vulnerabilities related to age and physical limitations, potentially indicating challenges in accessing resources or services. Similaly, Mobile Homes and Lack of Internet, reflect connectivity and housing stability issues, highlighting potential isolation or limited access to information and opportunities. Housing in buildings with 10 or more units contributes negatively, possibly indicating more stable or resource-rich environments. Individuals who speak English less than well also contribute negatively, suggesting they might be less associated with the vulnerabilities captured by PC2.

PC3 reveals significant positive contributions from factors such as adults (25+) without a high school diploma, uninsured individuals, youth (17 and younger), and those with limited English proficiency. These elements highlight educational and linguistic challenges that can impact economic opportunities and access to essential services. The presence of a substantial uninsured population further underscores potential barriers to healthcare, reflecting vulnerabilities in both economic stability and resource accessibility for these groups. Conversely, negative contributions come from lower unemployment rates among those aged 16 and older, reduced housing cost burdens, and living in large housing structures. These factors suggest more favorable economic conditions or shared living arrangements that alleviate individual financial stress. Additionally, the lack of vehicle availability might indicate reliance on public transportation, contributing to a reduced financial burden. These elements collectively suggest areas where stability and resource access might be stronger, contrasting with the vulnerabilities highlighted by the positive contributions.


1. There are several different ways to determine the number of principal components to retain.
One common method is to retain principal components that explain a certain percentage of the variance in the data.
    a. How many principal components are needed to explain 80% of the variance in the data?
    a. How many principal components are needed to explain 90% of the variance in the data?

```{python}
# Determine the number of principal components to retain based on explained variance
explained_variance_ratio = pca.explained_variance_ratio_
cumulative_variance = np.cumsum(explained_variance_ratio)
print(f"cumulative_variance = {cumulative_variance}")
```

Answer: Number of components needed to explain 80% of the variance: 7
Number of components needed to explain 90% of the variance: 11

1. An alternative approach is to plot the eigenvalues of the principal components and retain the components that are above the "elbow" in the plot. In other words the eigenvalues that are substantially larger than the rest.
    a. Create a [scree plot](https://en.wikipedia.org/wiki/Scree_plot) of the eigenvalues of the principal components.
    a. How many principal components should be retained based on the scree plot? This video may help: [PCA Scree Plot](https://youtu.be/vFUvNICWVz4?si=6NbyRcLRGT8L1HzI)

```{python}
# Create a scree plot of the eigenvalues of the principal components
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(explained_variance_ratio) + 1), pca.explained_variance_, marker='o', linestyle='--')
plt.xlabel('Principal Component Number')
plt.ylabel('Eigenvalue')
plt.title('Scree Plot of Principal Components')
plt.grid()
plt.show()
```
Answer: In the scree plot the explained variance starts to level off from PC4 and formed an elbow till PC6. So PC4-PC6 indicates a natural cutoff for the number of PCs to retain.

1. Cross-validation is another method to determine the number of principal components to retain.
This process requires some linear algebra that is beyond the scope of this course.
As such, I have written example [code](https://github.com/gabehassler/PRGS-Intro-to-ML-2024/blob/main/examples/pca_cross_validation.jl) in Julia that demonstrates how to perform cross-validation.
This procedure is a simplified versionof an approach explained in this [blog post](https://alexhwilliams.info/itsneuronalblog/2018/02/26/crossval/).
For the purposes of this assignment, the `pca_cv_error` function is a black box that returns the cross-validation error for a given number of principal components.
_Note: If you use a different programming language, you can use ChatGPT to translate the code to your language of choice._
    a. Compute the cross-validation error for 1 to 17 principal components. If this process is parallelizable, parallelize the code. If setting a random number seed would make this work more reproducible, set a random number seed.
    a. How many principal components should be retained based on the cross-validation error?

```{python}
# Used chatgpt to convert the provided Julia code to Python  

np.random.seed(42)  # Set random seed for reproducibility

def pca_approx(X, k):
    # Approximate the data matrix X using the first k principal components
    U, S, Vt = np.linalg.svd(X, full_matrices=False)
    Uk = U[:, :k]
    Sk = np.diag(S[:k])
    Vtk = Vt[:k, :]
    return Uk @ Sk @ Vtk

def pca_cv_error(X, k, folds):
    # Compute cross-validated error of approximating X using the first k principal components
    unique_folds = np.unique(folds)
    means = np.mean(X, axis=0)
    errors = []

    for fold in unique_folds:
        X_cv = X.copy()
        # Replace values for the current fold with column means (iterate through each feature)
        for i in range(X.shape[1]):
            X_cv[folds[:, i] == fold, i] = means[i]
        # Approximate X_cv using the first k components
        X_approx = pca_approx(X_cv, k)
        # Compute the error of the approximation for only the replaced values
        error = ((X_approx[folds == fold] - X[folds == fold]) ** 2).sum()
        errors.append(error)
    return np.mean(errors) / X.size
```

```{python}
# Prepare the data and folds for CV
n_folds = 20
folds = np.random.randint(1, n_folds + 1, size=standardized_data.shape)

# Compute CV error for 1 to 17 PCs
cross_val_errors = Parallel(n_jobs=-1)(             # -1 tells the program to use all available cores
    delayed(pca_cv_error)(standardized_data, k, folds) for k in range(1, 18)
)

# Plot the CV errors
plt.figure(figsize=(10, 6))
plt.plot(range(1, 18), cross_val_errors, marker='o', linestyle='--')
plt.xlabel('Number of Principal Components')
plt.ylabel('Cross-Validation Error')
plt.title('Cross-Validation Error for Different Numbers of Principal Components')
plt.grid()
plt.show()
```

Answer: Based on the scree plot, choosing 6 principal components (PCs) appears optimal because the plot shows a distinct "elbow" at 6 PCs. This suggests that additional components contribute less significantly to the overall variance.
